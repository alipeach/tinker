# Ultralytics ğŸš€ AGPL - 3.0 License - https://ultralytics.com/license

import json
from pathlib import Path
from typing import List, Optional, Union
import cv2
import numpy as np
from ultralytics import SAM, YOLO, RTDETR
from datetime import datetime, timedelta


def get_beijing_time():
    """
    è·å–æœ¬åœ°æ—¶é—´åŠ å…«å°æ—¶
    """
    local_time = datetime.now()
    beijing_time = local_time + timedelta(hours=8)
    return beijing_time


def convert_to_yolo_format(boxes, img_width, img_height, class_ids):
    """
    å°†æ£€æµ‹ç»“æœè½¬æ¢ä¸ºYOLOæ ‡æ³¨æ ¼å¼
    """
    yolo_lines = []
    for i, box in enumerate(boxes):
        x_center = (box[0] + box[2]) / (2 * img_width)
        y_center = (box[1] + box[3]) / (2 * img_height)
        width = (box[2] - box[0]) / img_width
        height = (box[3] - box[1]) / img_height
        yolo_lines.append(f"{class_ids[i]} {x_center} {y_center} {width} {height}")
    return yolo_lines


def split_and_detect(
    data: Union[str, Path],
    det_model: RTDETR,
    batch_dir: Path,
    device: str,
    conf: float,
    iou: float,
    imgsz: int,
    max_det: int,
    classes: Optional[List[int]],
    max_split_frames: int
):
    """
    æ‹†å¸§å¹¶è¿›è¡Œç›®æ ‡æ£€æµ‹
    """
    # åˆ›å»ºä¸åŒçš„å­ç›®å½•
    images_dir = batch_dir / "images"
    images_dir.mkdir(exist_ok=True)
    labels_dir = batch_dir / "labels"
    labels_dir.mkdir(exist_ok=True)

    # å¤„ç†è§†é¢‘æ–‡ä»¶ï¼Œæ‹†å¸§ä¿å­˜åŸå§‹æ–‡ä»¶
    cap = cv2.VideoCapture(str(data))
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))

    if max_split_frames == -1:
        max_split_frames = total_frames
    else:
        max_split_frames = min(max_split_frames, total_frames)

    frame_count = 0
    while cap.isOpened() and frame_count < max_split_frames:
        if frame_count % 100 == 0:
            print(f"å½“å‰æ‹†å¸§è¿›åº¦ï¼šå·²æ‹†åˆ† {frame_count} å¸§ï¼Œå…± {max_split_frames} å¸§ã€‚")
        ret, frame = cap.read()
        if not ret:
            break

        frame_count += 1

        # ä¿å­˜åŸå§‹å¸§
        raw_frame_path = images_dir / f"{frame_count:06d}.jpg"
        cv2.imwrite(str(raw_frame_path), frame)

    cap.release()
    print(f"æ‹†å¸§å®Œæˆï¼Œå…±æ‹†äº† {max_split_frames} å¸§ã€‚å³å°†å¼€å§‹æ£€æµ‹ã€‚")

    for frame_num in range(1, frame_count + 1):
        if frame_num % 100 == 0:
            print(f"å½“å‰æ£€æµ‹è¿›åº¦ï¼šå·²æ£€æµ‹ {frame_num} å¸§ï¼Œå…± {max_split_frames} å¸§ã€‚")

        raw_frame_path = images_dir / f"{frame_num:06d}.jpg"
        frame = cv2.imread(str(raw_frame_path))
        img_height, img_width = frame.shape[:2]

        # ä½¿ç”¨ detect æ–¹æ³•è¿›è¡Œç›®æ ‡æ£€æµ‹ï¼Œå…³é—­æ—¥å¿—è¾“å‡º
        result = det_model(frame, device=device, conf=conf, iou=iou, imgsz=imgsz, max_det=max_det, classes=classes, verbose=False)[0]
        class_ids = result.boxes.cls.int().tolist() if result.boxes.cls.numel() > 0 else []
        confidences = result.boxes.conf.cpu().numpy().tolist() if result.boxes.conf.numel() > 0 else []

        if class_ids:
            boxes = result.boxes.xyxy.cpu().numpy().tolist()

            # ä¿å­˜æ£€æµ‹ç»“æœåˆ°YOLOæ ‡æ³¨æ ¼å¼
            yolo_lines = convert_to_yolo_format(boxes, img_width, img_height, class_ids)
            label_path = labels_dir / f"{frame_num:06d}.txt"
            with open(label_path, 'w') as f:
                f.write('\n'.join(yolo_lines))
        else:
            # æ²¡æœ‰æ£€æµ‹ç»“æœï¼Œä¿å­˜ä¸ºç©ºæ–‡ä»¶
            label_path = labels_dir / f"{frame_num:06d}.txt"
            with open(label_path, 'w') as f:
                pass

    return frame_count


def convert_yolo_to_boxes(yolo_lines, img_width, img_height):
    """
    å°†YOLOæ ‡æ³¨æ ¼å¼è½¬æ¢ä¸ºè¾¹ç•Œæ¡†åæ ‡
    """
    boxes = []
    class_ids = []
    for line in yolo_lines:
        parts = line.strip().split()
        class_id = int(parts[0])
        x_center = float(parts[1])
        y_center = float(parts[2])
        width = float(parts[3])
        height = float(parts[4])

        x1 = (x_center - width / 2) * img_width
        y1 = (y_center - height / 2) * img_height
        x2 = (x_center + width / 2) * img_width
        y2 = (y_center + height / 2) * img_height

        boxes.append([x1, y1, x2, y2])
        class_ids.append(class_id)
    return boxes, class_ids


def segment_frames(
    data: Union[str, Path],
    sam_model: SAM,
    batch_dir: Path,
    device: str,
    num_sam_frames: int,
    max_split_frames: int
):
    """
    åŸºäºæ‹†å¸§å’Œæ£€æµ‹ç»“æœè¿›è¡Œåˆ†å‰²
    """
    cap = cv2.VideoCapture(str(data))
    fps = cap.get(cv2.CAP_PROP_FPS)
    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
    fourcc = cv2.VideoWriter_fourcc(*'mp4v')

    timestamp = get_beijing_time().strftime("%Y%m%d%H%M%S")
    output_video_path = batch_dir / f"{data.stem}_{timestamp}_segmented.mp4"
    out = cv2.VideoWriter(str(output_video_path), fourcc, fps, (width, height))

    if num_sam_frames == -1:
        num_sam_frames = max_split_frames
    else:
        num_sam_frames = min(num_sam_frames, max_split_frames)

    images_dir = batch_dir / "images"
    labels_dir = batch_dir / "labels"

    empty_frames = []
    for frame_num in range(1, num_sam_frames + 1):
        if frame_num % 100 == 0:
            print(f"å½“å‰åˆ†å‰²è¿›åº¦ï¼šå·²å¤„ç† {frame_num} å¸§ï¼Œå…± {num_sam_frames} å¸§ã€‚")

        raw_frame_path = images_dir / f"{frame_num:06d}.jpg"
        frame = cv2.imread(str(raw_frame_path))
        img_height, img_width = frame.shape[:2]

        # ä»æœ¬åœ°åŠ è½½æ£€æµ‹ç»“æœ
        label_path = labels_dir / f"{frame_num:06d}.txt"
        with open(label_path, 'r') as f:
            yolo_lines = f.readlines()

        boxes, class_ids = convert_yolo_to_boxes(yolo_lines, img_width, img_height)
        confidences = [1.0] * len(class_ids)  # å‡è®¾ç½®ä¿¡åº¦ä¸º1.0

        # å¦‚æœå½“å‰å¸§æ— æ£€æµ‹ç»“æœï¼Œå–å‰å5å¸§ä¸­æœ‰ç»“æœçš„ä¸€å¸§
        if not class_ids:
            for offset in range(1, 30):
                prev_frame_num = frame_num - offset
                next_frame_num = frame_num + offset
                prev_result = None
                next_result = None

                if prev_frame_num > 0:
                    prev_label_path = labels_dir / f"{prev_frame_num:06d}.txt"
                    with open(prev_label_path, 'r') as f:
                        prev_yolo_lines = f.readlines()
                        prev_boxes, prev_class_ids = convert_yolo_to_boxes(prev_yolo_lines, img_width, img_height)
                        if prev_class_ids:
                            prev_result = {"boxes": prev_boxes, "class_ids": prev_class_ids}
                if next_frame_num <= max_split_frames:
                    next_label_path = labels_dir / f"{next_frame_num:06d}.txt"
                    with open(next_label_path, 'r') as f:
                        next_yolo_lines = f.readlines()
                        next_boxes, next_class_ids = convert_yolo_to_boxes(next_yolo_lines, img_width, img_height)
                        if next_class_ids:
                            next_result = {"boxes": next_boxes, "class_ids": next_class_ids}

                if prev_result and prev_result["class_ids"]:
                    boxes = prev_result["boxes"]
                    class_ids = prev_result["class_ids"]
                    break
                elif next_result and next_result["class_ids"]:
                    boxes = next_result["boxes"]
                    class_ids = next_result["class_ids"]
                    break

        rendered_frame = frame
        if class_ids:
            if empty_frames:
                print(f"å¸§ {', '.join(map(str, empty_frames))} æ— æ£€æµ‹ç»“æœ")
                empty_frames = []
            boxes = np.array(boxes)
            sam_results = sam_model(frame, bboxes=boxes, verbose=False, save=False, device=device)
            segments = sam_results[0].masks.xyn

            # Render the masks on a new image with transparency
            rendered_frame = frame.copy()
            h, w = rendered_frame.shape[:2]
            overlay = rendered_frame.copy()
            alpha = 0.4  # é€æ˜åº¦ï¼ŒèŒƒå›´ä» 0 åˆ° 1ï¼Œå€¼è¶Šå°è¶Šé€æ˜


            #ç´«è‰²
            # color = (128, 0, 128)

             #ç´«è‰²-2
            color=(204,0,153)
            #ç»¿è‰²
            # color = (0, 255, 0)
            #è‰ç»¿è‰²
            # color = (124, 252, 0)

            for segment in segments:
                segment = (segment * np.array([w, h])).astype(np.int32)
                try:
                    cv2.fillPoly(overlay, [segment], color = color)
                except cv2.error as e:
                    print(f"å¼‚å¸¸ä¿¡æ¯ï¼šç¬¬ {frame_num} å¸§ï¼ŒfillPolyæ‰§è¡Œå¼‚å¸¸: {e}")

            # å°†è¦†ç›–å±‚ä¸åŸå›¾æ··åˆ
            cv2.addWeighted(overlay, alpha, rendered_frame, 1 - alpha, 0, rendered_frame)

        else:
            empty_frames.append(frame_num)
            if len(empty_frames) == 10:
                print(f"å¸§ {', '.join(map(str, empty_frames))} æ— æ£€æµ‹ç»“æœ")
                empty_frames = []

        out.write(rendered_frame)

    if empty_frames:
        print(f"å¸§ {', '.join(map(str, empty_frames))} æ— æ£€æµ‹ç»“æœ")

    out.release()
    cap.release()


def auto_annotate(
    data: Union[str, Path],
    det_model: str = "/workspace/yolo11x.pt",
    sam_model: str = "/workspace/sam_b.pt",
    device: str = "cuda:0",
    conf: float = 0.25,
    iou: float = 0.45,
    imgsz: int = 640,
    max_det: int = 3,
    classes: Optional[List[int]] = [15],
    output_dir: Optional[Union[str, Path]] = "/workspace/outputs",
    num_sam_frames: int = -1,
    max_split_frames: int = -1,
    mode: str = "detect_and_segment"
) -> None:
    """
    Automatically annotate a video using a YOLO object detection model and a SAM segmentation model.

    This function processes frames of a video, detects objects using a YOLO model,
    and then generates segmentation masks using a SAM model. The resulting masks are rendered on a new video.

    Args:
        data (str | Path): Path to a video file to be annotated.
        det_model (str): Path or name of the pre - trained YOLO detection model.
        sam_model (str): Path or name of the pre - trained SAM segmentation model.
        device (str): Device to run the models on (e.g., 'cpu', 'cuda', '0').
        conf (float): Confidence threshold for detection model.
        iou (float): IoU threshold for filtering overlapping boxes in detection results.
        imgsz (int): Input image resize dimension.
        max_det (int): Maximum number of detections per image.
        classes (List[int] | None): Filter predictions to specified class IDs, returning only relevant detections.
        output_dir (str | Path | None): Directory to save the annotated results. If None, a default directory is created.
        num_sam_frames (int): Number of frames to process for SAM segmentation after detection. -1 means process all detected frames.
        max_split_frames (int): Maximum number of frames to split. -1 means split all frames.
        mode (str): Mode of operation. Can be 'detect', 'segment', or 'detect_and_segment'.
    """
    start_time = datetime.now()
    det_model = RTDETR(det_model)
    sam_model = SAM(sam_model)

    data = Path(data)
    if not output_dir:
        output_dir = data.parent / f"{data.stem}_auto_annotate_labels"
    output_dir = Path(output_dir)

    # è·å–æœ¬åœ°æ—¶é—´åŠ å…«å°æ—¶å¹¶ç”Ÿæˆæ‰¹æ¬¡å·ç›®å½•ï¼Œç²¾ç¡®åˆ°åˆ†é’Ÿ
    beijing_time = get_beijing_time()
    batch_dir_name = beijing_time.strftime("%Y-%m-%d_%H-%M")
    batch_dir = output_dir / batch_dir_name
    batch_dir.mkdir(exist_ok=True, parents=True)

    frame_count = 0
    if mode in ["detect", "detect_and_segment"]:
        # æ‹†å¸§å¹¶æ£€æµ‹
        frame_count = split_and_detect(
            data, det_model, batch_dir, device, conf, iou, imgsz, max_det, classes, max_split_frames
        )

    if mode in ["segment", "detect_and_segment"]:
        if frame_count == 0:
            # å¦‚æœåªè¿›è¡Œåˆ†å‰²ï¼Œéœ€è¦è®¡ç®—æœ€å¤§å¸§æ•°
            cap = cv2.VideoCapture(str(data))
            total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
            if max_split_frames == -1:
                frame_count = total_frames
            else:
                frame_count = min(max_split_frames, total_frames)
            cap.release()
        # åˆ†å‰²
        segment_frames(data, sam_model, batch_dir, device, num_sam_frames, frame_count)

    end_time = datetime.now()
    elapsed_time = end_time - start_time
    log_message = f"ç¨‹åºæ‰§è¡Œå®Œæˆï¼Œå¤„ç†äº† {frame_count} å¸§ï¼Œè€—æ—¶ {elapsed_time}ã€‚"
    print(log_message)


# æ‰§è¡Œé€»è¾‘
# 1ã€åŸè§†é¢‘é€å¸§æ‹†è§£ï¼Œå¹¶ä¿å­˜åœ¨æœ¬åœ°
# 2ã€ä½¿ç”¨æ£€æµ‹æ¨¡å‹ï¼Œå–æœ¬åœ°å¸§è¿›è¡Œæ£€æµ‹ï¼Œæ£€æµ‹ç»“æœæ–‡æœ¬ã€æ¸²æŸ“åçš„å›¾åƒä¿å­˜åœ¨æœ¬åœ°
# 3ã€åˆ¤æ–­æ˜¯å¦å­˜åœ¨è¿ç»­å¸§é‡Œé¢æœ‰æ¼æ£€çš„æƒ…å†µï¼Œå¯¹æ¼æ£€å¸§è¿›è¡Œæ£€æµ‹ç»“æœèåˆï¼Œå¹¶æ›´æ–°æœ¬åœ°æ£€æµ‹ç»“æœ
# 4ã€å–æœ¬åœ°æ£€æµ‹ç»“æœï¼Œè¿›è¡Œåˆ†å‰²ï¼Œå¹¶ä¿å­˜ä¸ºè§†é¢‘æ–‡ä»¶
# 5ã€æ”¯æŒè°ƒè¯•ï¼Œå¯ä»¥è®¾ç½®åªæ‹†éƒ¨åˆ†å¸§å’Œå…¨é‡å¸§
if __name__ == "__main__":
    # è¯·æ ¹æ®å®é™…æƒ…å†µä¿®æ”¹è¿™äº›å‚æ•°
    # data_path = "/workspace/çŒ«å²›çš„çŒ«-5.mp4"
    # output_dir = "/workspace/outputs/çŒ«å²›çš„çŒ«-5"
    
    # data_path = "/workspace/çŒ«å²›çš„çŒ«-6.mp4"
    # output_dir = "/workspace/outputs/çŒ«å²›çš„çŒ«-6"

    # data_path = "/workspace/è¥¿æ¹–çš„æ¾é¼ -01.mp4"
    # output_dir = "/workspace/outputs/è¥¿æ¹–çš„æ¾é¼ -01"

    # data_path = "/workspace/cat-flight.mp4"
    # output_dir = "/workspace/outputs/cat-flight"

    data_path = "/workspace/white-fox.mp4"
    output_dir = "/workspace/outputs/white-fox"
    
    det_model_name = "/workspace/model/rtdetr-x.pt"
    sam_model_name = "/workspace/model/sam2_b.pt"
    # conf = 0.20
    # classes = [15,16,21]

    conf = 0.25
    classes = [16]
    # æ‹†å¸§å¹¶æ£€æµ‹åï¼Œéœ€è¦è¿›è¡ŒSAMå¤„ç†çš„æœ€å¤§å¸§æ•°
    num_sam_frames = -1
    # ç›´æ¥æŒ‡å®šæœ€å¤šæ‹†å¤šå°‘å¸§
    max_split_frames = -1
    # é€‰æ‹©æ¨¡å¼ï¼š'detect', 'segment', 'detect_and_segment'
    mode = 'detect_and_segment'

    auto_annotate(data=data_path, det_model=det_model_name, sam_model=sam_model_name, output_dir=output_dir,
                  conf=conf, classes=classes, num_sam_frames=num_sam_frames, max_split_frames=max_split_frames, mode=mode)
    