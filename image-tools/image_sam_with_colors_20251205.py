# Ultralytics ğŸš€ AGPL - 3.0 License - https://ultralytics.com/license

import json
from pathlib import Path
from typing import List, Optional, Union, Tuple
import cv2
import numpy as np
from ultralytics import SAM  # ç§»é™¤YOLOå¯¼å…¥ï¼ˆä»…ä¿ç•™SAMï¼‰
from datetime import datetime, timedelta


def get_beijing_time():
    """è·å–æœ¬åœ°æ—¶é—´åŠ å…«å°æ—¶"""
    local_time = datetime.now()
    beijing_time = local_time + timedelta(hours=8)
    return beijing_time


def convert_yolo_to_boxes(yolo_lines, img_width, img_height):
    """å°†YOLOæ ‡æ³¨æ ¼å¼è½¬æ¢ä¸ºè¾¹ç•Œæ¡†åæ ‡ï¼ˆä»detect_and_sam-1024ç§»æ¤ï¼‰"""
    boxes, class_ids = [], []
    for line in yolo_lines:
        parts = line.strip().split()
        if not parts:
            continue
        class_id = int(parts[0])
        x_center, y_center = float(parts[1]), float(parts[2])
        width, height = float(parts[3]), float(parts[4])

        x1 = (x_center - width / 2) * img_width
        y1 = (y_center - height / 2) * img_height
        x2 = (x_center + width / 2) * img_width
        y2 = (y_center + height / 2) * img_height

        boxes.append([x1, y1, x2, y2])
        class_ids.append(class_id)
    return boxes, class_ids


def segment_image(
        img_path: Union[str, Path],
        ann_path: Union[str, Path],  # æ”¹ä¸ºå¿…ä¼ å‚æ•°ï¼ˆå¿…é¡»æä¾›æ ‡æ³¨æ–‡ä»¶ï¼‰
        sam_model: str = "sam2_l.pt",
        output_dir: Path = Path("segment_outputs"),
        device: str = "cuda:0",
        filter_colors: Union[Tuple[int, int, int], List[Tuple[int, int, int]]] = [(0, 255, 0)],
        alpha: float = 0.3
) -> None:
    """
    ç›´æ¥åŸºäºæ ‡æ³¨æ–‡ä»¶çš„è¾¹ç•Œæ¡†è°ƒç”¨SAMè¿›è¡Œåˆ†å‰²ï¼Œä¸ºæ¯ç§é¢œè‰²ç”Ÿæˆç‹¬ç«‹ç»“æœ

    æ ¸å¿ƒé€»è¾‘å˜æ›´ï¼š
    1. ç§»é™¤YOLOæ£€æµ‹é€»è¾‘ï¼Œå®Œå…¨ä¾èµ–æ ‡æ³¨æ–‡ä»¶ç”Ÿæˆè¾¹ç•Œæ¡†
    2. æ ‡æ³¨æ–‡ä»¶ä¸ºå¿…ä¼ å‚æ•°ï¼Œæ— æ ‡æ³¨æ–‡ä»¶åˆ™ç›´æ¥æŠ¥é”™
    3. åŸºäºæ ‡æ³¨è¾¹ç•Œæ¡†è°ƒç”¨SAMè¿›è¡Œç²¾ç¡®åˆ†å‰²
    4. ä¿ç•™å¤šé¢œè‰²æ¸²æŸ“åŠŸèƒ½
    """
    # ç¡®ä¿filter_colorsæ˜¯åˆ—è¡¨æ ¼å¼
    if isinstance(filter_colors, tuple):
        filter_colors = [filter_colors]

    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir.mkdir(exist_ok=True, parents=True)

    # è¯»å–å›¾åƒ
    img = cv2.imread(str(img_path))
    if img is None:
        raise ValueError(f"æ— æ³•è¯»å–å›¾ç‰‡: {img_path}")
    img_height, img_width = img.shape[:2]

    # 1. ä»æ ‡æ³¨æ–‡ä»¶è§£æè¾¹ç•Œæ¡†ï¼ˆæ ¸å¿ƒï¼šæ›¿ä»£YOLOæ£€æµ‹é€»è¾‘ï¼‰
    if not Path(ann_path).exists():
        raise FileNotFoundError(f"æ ‡æ³¨æ–‡ä»¶ä¸å­˜åœ¨: {ann_path}")

    with open(ann_path, 'r') as f:
        yolo_lines = f.readlines()

    boxes, class_ids = convert_yolo_to_boxes(yolo_lines, img_width, img_height)
    if not boxes:
        raise ValueError(f"æ ‡æ³¨æ–‡ä»¶ä¸­æœªè§£æåˆ°æœ‰æ•ˆè¾¹ç•Œæ¡†: {ann_path}")

    # è½¬æ¢ä¸ºnumpyæ•°ç»„ï¼ˆé€‚é…SAMè¾“å…¥æ ¼å¼ï¼‰
    boxes_np = np.array(boxes)

    # 2. è°ƒç”¨SAMè¿›è¡Œåˆ†å‰²ï¼ˆåŸºäºæ ‡æ³¨æ–‡ä»¶çš„è¾¹ç•Œæ¡†ï¼‰
    sam = SAM(sam_model)
    sam_results = sam(img, bboxes=boxes_np, verbose=False, save=False, device=device)
    segments = sam_results[0].masks.xyn  # è·å–å½’ä¸€åŒ–çš„åˆ†å‰²æ©ç 

    # å¤„ç†æ— åˆ†å‰²ç»“æœçš„æƒ…å†µ
    if not segments:
        print("åŸºäºæ ‡æ³¨è¾¹ç•Œæ¡†æœªæ£€æµ‹åˆ°å¯åˆ†å‰²çš„ç›®æ ‡")
        for filter_color in filter_colors:
            r, g, b = filter_color
            color_suffix = f"({r},{g},{b})"
            output_path = output_dir / f"{img_path.stem}_segmented_{color_suffix}.jpg"
            cv2.imwrite(str(output_path), img)
        return

    # 3. å¤šé¢œè‰²æ¸²æŸ“ï¼ˆä¿ç•™åŸå¤šé¢œè‰²åŠŸèƒ½ï¼‰
    for index, filter_color in enumerate(filter_colors):
        rendered_img = img.copy()
        overlay = rendered_img.copy()

        # å¯¹æ¯ä¸ªåˆ†å‰²åŒºåŸŸåº”ç”¨å½“å‰é¢œè‰²
        for segment in segments:
            # å°†å½’ä¸€åŒ–åæ ‡è½¬æ¢ä¸ºå›¾åƒå®é™…åæ ‡
            segment_coords = (segment * np.array([img_width, img_height])).astype(np.int32)
            # RGBè½¬BGRï¼ˆOpenCVé¢œè‰²é€šé“æ ¼å¼ï¼‰
            bgr_color = (filter_color[2], filter_color[1], filter_color[0])
            cv2.fillPoly(overlay, [segment_coords], color=bgr_color)

        # æ··åˆåŸå›¾å’Œåˆ†å‰²å±‚
        cv2.addWeighted(overlay, alpha, rendered_img, 1 - alpha, 0, rendered_img)

        # ä¿å­˜ç»“æœ
        r, g, b = filter_color
        color_suffix = f"({r},{g},{b})"
        output_path = output_dir / f"{img_path.stem}-{index}-{color_suffix}.jpg"
        cv2.imwrite(str(output_path), rendered_img)
        print(f"åˆ†å‰²ç»“æœå·²ä¿å­˜è‡³: {output_path}")


def process_single_image(
        img_path: Union[str, Path],
        ann_path: Union[str, Path],  # æ”¹ä¸ºå¿…ä¼ å‚æ•°
        sam_model: str = "sam2_l.pt",
        device: str = "cuda:0",
        output_dir: Optional[Union[str, Path]] = None,
        filter_colors: Union[Tuple[int, int, int], List[Tuple[int, int, int]]] = [(0, 255, 0)],
        alpha: float = 0.3
) -> None:
    """å¤„ç†å•å¼ å›¾ç‰‡çš„ä¸»å‡½æ•°ï¼Œç›´æ¥åŸºäºæ ‡æ³¨æ–‡ä»¶è¾¹ç•Œæ¡†+SAMåˆ†å‰²+å¤šé¢œè‰²æ¸²æŸ“"""
    start_time = datetime.now()
    img_path = Path(img_path)
    ann_path = Path(ann_path)

    if output_dir is None:
        output_dir = img_path.parent / f"{img_path.stem}_segment_results"
    output_dir = Path(output_dir)

    # æ‰§è¡Œåˆ†å‰²
    segment_image(
        img_path=img_path,
        ann_path=ann_path,
        sam_model=sam_model,
        output_dir=output_dir,
        device=device,
        filter_colors=filter_colors,
        alpha=alpha
    )

    end_time = datetime.now()
    elapsed_time = end_time - start_time
    print(f"æ‰€æœ‰é¢œè‰²å¤„ç†å®Œæˆï¼Œè€—æ—¶ {elapsed_time.total_seconds():.2f} ç§’")


if __name__ == "__main__":

    # ç¤ºä¾‹å‚æ•°
    # è¾“å…¥å›¾ç‰‡è·¯å¾„
    img_path = "xxx"
    # é»˜è®¤ä¸ºåŒè·¯å¾„ä¸‹åŒå.txtæ–‡ä»¶  
    ann_path = "xxx" 

    #åˆ†å‰²æ¨¡å‹è·¯å¾„
    sam_model_path = "xxx"
    # è¾“å‡ºç›®å½•
    output_dir = "xxx"  
    device = "cuda:0"  

    filter_colors = [
        (220, 20, 60),  # é²œæ˜çš„çº¢è‰²ï¼Œç”¨äºç¬¬ä¸€ç±»ç›®æ ‡
        (119, 11, 32),  # æ·±çº¢è‰²ï¼Œä¸çº¢è‰²æœ‰åŒºåˆ†ï¼Œå¯ç”¨äºç¬¬äºŒç±»
        (0, 0, 255),  # è“è‰²ï¼Œé€‚åˆç¬¬ä¸‰ç±»
        (0, 255, 0),  # ç»¿è‰²ï¼Œä»£è¡¨ç¬¬å››ç±»
        (255, 255, 0),  # é»„è‰²ï¼Œç”¨äºç¬¬äº”ç±»
        (255, 165, 0),  # æ©™è‰²ï¼Œç¬¬å…­ç±»
        (128, 0, 128),  # ç´«è‰²ï¼Œç¬¬ä¸ƒç±»
        (255, 0, 255),  # å“çº¢è‰²ï¼Œç¬¬å…«ç±»
        (0, 255, 255),  # é’è‰²ï¼Œç¬¬ä¹ç±»
        (139, 69, 19),  # æ£•è‰²ï¼Œç¬¬åç±»
        (127, 255, 212),  # æµ…è“ç»¿è‰²ï¼Œç¬¬åä¸€ç±»
        (144, 238, 144),  # æ·¡ç»¿è‰²ï¼Œç¬¬åäºŒç±»
        (255, 105, 180),  # æµ…ç²‰çº¢è‰²ï¼Œç¬¬åä¸‰ç±»
        (240, 230, 140),  # ç±³è‰²ï¼Œç¬¬åå››ç±»
        (255, 228, 181),  # æµ…é»„è‰²ï¼Œç¬¬åäº”ç±»
        (173, 255, 47),  # äº®ç»¿è‰²åé»„ï¼Œç¬¬åå…­ç±»
        (100, 149, 237),  # æ·¡è“è‰²ï¼Œç¬¬åä¸ƒç±»
        (218, 112, 214),  # æµ…ç´«è‰²ï¼Œç¬¬åå…«ç±»
        (199, 21, 133),  # æ·±ç²‰è‰²ï¼Œç¬¬åä¹ç±»
        (255, 248, 220)  # è±¡ç‰™è‰²ï¼Œç¬¬äºŒåç±»
    ]

    alpha = 0.4

    # æ‰§è¡Œå¤„ç†
    process_single_image(
        img_path=img_path,
        ann_path=ann_path,
        sam_model=sam_model_path,
        device=device,
        output_dir=output_dir,
        filter_colors=filter_colors,
        alpha=alpha
    )