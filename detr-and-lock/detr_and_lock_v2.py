# Ultralytics ğŸš€ AGPL - 3.0 License - https://ultralytics.com/license

#yolo track model=/Users/apple/Work/models - yolo11/yolo11x.pt source=çŒ«å²›çš„çŒ« - 5.mp4 device=mps show=true

from pathlib import Path
from typing import List, Optional, Union
import cv2
import numpy as np
from ultralytics import RTDETR
import datetime


def is_close(box1, box2, threshold=2.5):
    """
    åˆ¤æ–­ä¸¤ä¸ªbboxæ˜¯å¦ä½ç½®ç›¸è¿‘
    """
    center1 = ((box1[0] + box1[2]) / 2, (box1[1] + box1[3]) / 2)
    center2 = ((box2[0] + box2[2]) / 2, (box2[1] + box2[3]) / 2)
    distance = np.sqrt((center1[0] - center2[0]) ** 2 + (center1[1] - center2[1]) ** 2)
    avg_width = (box1[2] - box1[0] + box2[2] - box2[0]) / 2
    return distance / avg_width < threshold


def interpolate_box(box1, box2, num_frames, frame_index):
    """
    å¯¹æ¼æ£€çš„å¸§è¿›è¡Œé¢„æµ‹
    """
    x1 = box1[0] + (box2[0] - box1[0]) * frame_index / (num_frames + 1)
    y1 = box1[1] + (box2[1] - box1[1]) * frame_index / (num_frames + 1)
    x2 = box1[2] + (box2[2] - box1[2]) * frame_index / (num_frames + 1)
    y2 = box1[3] + (box2[3] - box1[3]) * frame_index / (num_frames + 1)
    return [x1, y1, x2, y2]


def auto_annotate(
    data: Union[str, Path],
    det_model: str = "/workspace/yolo11x.pt",
    device: str = "cuda:0",
    conf: float = 0.25,
    iou: float = 0.45,
    # iou: float = 0.75,
    imgsz: int = 640,
    max_det: int = 1,
    classes: Optional[List[int]] = [15],
    output_dir: Optional[Union[str, Path]] = "/workspace/outputs",
) -> None:
    """
    Automatically annotate a video using a YOLO object detection model.

    This function processes frames of a video, detects objects using a YOLO model,
    and then generates a new video with crosshair lock boxes on detected objects.

    Args:
        data (str | Path): Path to a video file to be annotated.
        det_model (str): Path or name of the pre - trained YOLO detection model.
        device (str): Device to run the models on (e.g., 'cpu', 'cuda', '0').
        conf (float): Confidence threshold for detection model.
        iou (float): IoU threshold for filtering overlapping boxes in detection results.
        imgsz (int): Input image resize dimension.
        max_det (int): Maximum number of detections per image.
        classes (List[int] | None): Filter predictions to specified class IDs, returning only relevant detections.
        output_dir (str | Path | None): Directory to save the annotated results. If None, a default directory is created.

    Examples:
        >>> from ultralytics.data.annotator import auto_annotate
        >>> auto_annotate(data="video.mp4", det_model="yolo11n.pt")
    """
    det_model = RTDETR(det_model)

    data = Path(data)
    if not output_dir:
        output_dir = data.parent / f"{data.stem}_auto_annotate_labels"
    output_dir = Path(output_dir)
    output_dir.mkdir(exist_ok=True, parents=True)

    # åˆ›å»ºlabelç›®å½•
    label_dir = output_dir / "label"
    label_dir.mkdir(exist_ok=True, parents=True)

    # å¤„ç†è§†é¢‘æ–‡ä»¶
    cap = cv2.VideoCapture(str(data))
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    fps = cap.get(cv2.CAP_PROP_FPS)
    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
    fourcc = cv2.VideoWriter_fourcc(*'mp4v')

    # æ ¹æ®æ—¶é—´æˆ³ç”Ÿæˆæ–°çš„æ–‡ä»¶å
    timestamp = datetime.datetime.now().strftime("%Y%m%d%H%M%S")
    output_video_path = output_dir / f"{data.stem}_{timestamp}_processed.mp4"
    out = cv2.VideoWriter(str(output_video_path), fourcc, fps, (width, height))

    # ä¿å­˜æ¯ä¸€å¸§çš„æ£€æµ‹ç»“æœ
    all_results = []
    frame_count = 0
    while cap.isOpened():
        ret, frame = cap.read()
        if not ret:
            break

        frame_count += 1
        if frame_count % 100 == 0:
            print(f"æ­£åœ¨å¤„ç†ç¬¬ {frame_count} å¸§ï¼Œæ€»å…± {total_frames} å¸§")
        # ä½¿ç”¨ detect æ–¹æ³•
        result = det_model(frame, device=device, conf=conf, iou=iou, imgsz=imgsz, max_det=max_det, classes=classes,
                           verbose=False)[0]
        class_ids = result.boxes.cls.int().tolist()  # noqa
        boxes = result.boxes.xyxy.cpu().numpy().astype(int) if len(result.boxes) > 0 else []
        all_results.append((class_ids, boxes))

    # å¤„ç†æ¼æ£€çš„å¸§
    for i in range(len(all_results)):
        if len(all_results[i][1]) == 0:
            # å‘å‰æŸ¥æ‰¾æœ€è¿‘çš„æ£€æµ‹ç»“æœ
            prev_index = None
            for j in range(i - 1, max(-1, i - 15), -1):
                if len(all_results[j][1]) > 0:
                    prev_index = j
                    break
            # å‘åæŸ¥æ‰¾æœ€è¿‘çš„æ£€æµ‹ç»“æœ
            next_index = None
            for j in range(i + 1, min(len(all_results), i + 15)):
                if len(all_results[j][1]) > 0:
                    next_index = j
                    break
            if prev_index is not None and next_index is not None:
                prev_box = all_results[prev_index][1][0]
                next_box = all_results[next_index][1][0]
                if is_close(prev_box, next_box):
                    num_frames = next_index - prev_index - 1
                    frame_index = i - prev_index
                    interpolated_box = interpolate_box(prev_box, next_box, num_frames, frame_index)
                    # è€ƒè™‘å‰åå¸§è¾¹ç•Œé—®é¢˜
                    interpolated_box = [max(0, coord) for coord in interpolated_box]
                    interpolated_box[2] = min(width, interpolated_box[2])
                    interpolated_box[3] = min(height, interpolated_box[3])
                    all_results[i] = (all_results[prev_index][0], [np.array(interpolated_box).astype(int)])
                    print(f"ç¬¬ {i + 1} å¸§æ¼æ£€ï¼Œå·²æ ¹æ®ç¬¬ {prev_index + 1} å¸§å’Œç¬¬ {next_index + 1} å¸§é¢„æµ‹æ£€æµ‹ç»“æœ")

    # æ¸²æŸ“çŸ©å½¢æ¡†å’Œå†›äº‹ç„å‡†çº¿
    cap.set(cv2.CAP_PROP_POS_FRAMES, 0)
    frame_count = 0
    while cap.isOpened():
        ret, frame = cap.read()
        if not ret:
            break

        frame_count += 1
        if frame_count % 100 == 0:
            print(f"æ­£åœ¨æ¸²æŸ“ç¬¬ {frame_count} å¸§çš„çŸ©å½¢æ¡†å’Œå†›äº‹ç„å‡†çº¿ï¼Œæ€»å…± {total_frames} å¸§")
        class_ids, boxes = all_results[frame_count - 1]

        # ä¿å­˜æ£€æµ‹ç»“æœåˆ°labelç›®å½•
        label_file_path = label_dir / f"{frame_count:06d}.txt"
        with open(label_file_path, 'w') as f:
            for i, box in enumerate(boxes):
                x1, y1, x2, y2 = box
                class_id = class_ids[i]
                f.write(f"{class_id} {x1} {y1} {x2} {y2}\n")

        # æ¸²æŸ“çŸ©å½¢æ¡†å’Œå†›äº‹ç„å‡†çº¿
        rendered_frame = frame.copy()
        for box in boxes:
            x1, y1, x2, y2 = box
            center_x = (x1 + x2) // 2
            center_y = (y1 + y2) // 2

            # æ§åˆ¶å››ä¸ªç›´è§’å¤§å°æ¯”ä¾‹çš„å› å­
            corner_ratio = 0.3  # å¯ä»¥æ ¹æ®éœ€è¦è°ƒæ•´è¿™ä¸ªæ¯”ä¾‹
            corner_length_x = int((x2 - x1) * corner_ratio)
            corner_length_y = int((y2 - y1) * corner_ratio)

            # ç»˜åˆ¶çŸ©å½¢æ¡†ï¼ˆåªå±•ç¤ºå››ä¸ªç›´è§’ï¼‰
            rectangle_color = (0, 255, 0)  # ç»¿è‰²
            cv2.line(rendered_frame, (x1, y1), (x1 + corner_length_x, y1), rectangle_color, 2)
            cv2.line(rendered_frame, (x1, y1), (x1, y1 + corner_length_y), rectangle_color, 2)
            cv2.line(rendered_frame, (x2, y1), (x2 - corner_length_x, y1), rectangle_color, 2)
            cv2.line(rendered_frame, (x2, y1), (x2, y1 + corner_length_y), rectangle_color, 2)
            cv2.line(rendered_frame, (x1, y2), (x1 + corner_length_x, y2), rectangle_color, 2)
            cv2.line(rendered_frame, (x1, y2), (x1, y2 - corner_length_y), rectangle_color, 2)
            cv2.line(rendered_frame, (x2, y2), (x2 - corner_length_x, y2), rectangle_color, 2)
            cv2.line(rendered_frame, (x2, y2), (x2, y2 - corner_length_y), rectangle_color, 2)

            # ç»˜åˆ¶åå­—çº¿ï¼ˆè™šçº¿ï¼‰
            crosshair_color = (128, 128, 128)  # æ¯”çŸ©å½¢æ¡†é¢œè‰²æµ…çš„é»‘è‰²
            dash_length = 10
            gap_length = 10
            # ç¼©å°åå­—çº¿çš„èŒƒå›´ï¼Œä¾‹å¦‚ç¼©å°ä¸ºåŸæ¥çš„0.8å€
            shrink_factor = 0.5
            new_x1 = int(center_x - (center_x - x1) * shrink_factor)
            new_x2 = int(center_x + (x2 - center_x) * shrink_factor)
            new_y1 = int(center_y - (center_y - y1) * shrink_factor)
            new_y2 = int(center_y + (y2 - center_y) * shrink_factor)

            for i in range(new_y1, new_y2, dash_length + gap_length):
                end_y = min(i + dash_length, new_y2)
                cv2.line(rendered_frame, (center_x, i), (center_x, end_y), crosshair_color, 2)
            for i in range(new_x1, new_x2, dash_length + gap_length):
                end_x = min(i + dash_length, new_x2)
                cv2.line(rendered_frame, (i, center_y), (end_x, center_y), crosshair_color, 2)

        out.write(rendered_frame)

    cap.release()
    out.release()


if __name__ == "__main__":
    # è¯·æ ¹æ®å®é™…æƒ…å†µä¿®æ”¹è¿™äº›å‚æ•°
    data_path = "/workspace/fpvè¿½èˆ¹.mp4"
    output_dir = "/workspace/outputs/fpvè¿½èˆ¹"
    det_model_name = "/workspace/model/rtdetr-x.pt"
    # det_model_name = "/workspace/model/yolo11x.pt"
    classes = [8]
    conf = 0.75
    auto_annotate(data=data_path, det_model=det_model_name, output_dir=output_dir, classes=classes, conf=conf)