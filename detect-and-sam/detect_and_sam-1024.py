# Ultralytics ğŸš€ AGPL - 3.0 License - https://ultralytics.com/license

import cv2
import numpy as np
from pathlib import Path
from typing import List, Optional, Union
from datetime import datetime, timedelta
from ultralytics import SAM, YOLO


def get_beijing_time():
    """è·å–æœ¬åœ°æ—¶é—´åŠ å…«å°æ—¶ï¼ˆåŒ—äº¬æ—¶é—´ï¼‰"""
    return datetime.now() + timedelta(hours=8)


def convert_to_yolo_format(boxes, img_width, img_height, class_ids):
    """å°†æ£€æµ‹ç»“æœè½¬æ¢ä¸ºYOLOæ ‡æ³¨æ ¼å¼"""
    yolo_lines = []
    for i, box in enumerate(boxes):
        x_center = (box[0] + box[2]) / (2 * img_width)
        y_center = (box[1] + box[3]) / (2 * img_height)
        width = (box[2] - box[0]) / img_width
        height = (box[3] - box[1]) / img_height
        yolo_lines.append(f"{class_ids[i]} {x_center} {y_center} {width} {height}")
    return yolo_lines


def convert_yolo_to_boxes(yolo_lines, img_width, img_height):
    """å°†YOLOæ ‡æ³¨æ ¼å¼è½¬æ¢ä¸ºè¾¹ç•Œæ¡†åæ ‡"""
    boxes, class_ids = [], []
    for line in yolo_lines:
        parts = line.strip().split()
        class_id = int(parts[0])
        x_center, y_center = float(parts[1]), float(parts[2])
        width, height = float(parts[3]), float(parts[4])

        x1 = (x_center - width / 2) * img_width
        y1 = (y_center - height / 2) * img_height
        x2 = (x_center + width / 2) * img_width
        y2 = (y_center + height / 2) * img_height

        boxes.append([x1, y1, x2, y2])
        class_ids.append(class_id)
    return boxes, class_ids


def split_and_detect(
    video_path: Union[str, Path],
    det_model: YOLO,
    output_dir: Path,
    device: str,
    conf: float,
    iou: float,
    imgsz: int,
    max_det: int,
    classes: Optional[List[int]],
    max_split_frames: int
):
    """æ‹†å¸§å¹¶è¿›è¡Œç›®æ ‡æ£€æµ‹ï¼Œè¿”å›å¤„ç†çš„å¸§æ•°"""
    # åˆ›å»ºä¿å­˜ç›®å½•
    images_dir = output_dir / "images"
    labels_dir = output_dir / "labels"
    images_dir.mkdir(exist_ok=True)
    labels_dir.mkdir(exist_ok=True)

    # è§†é¢‘æ‹†å¸§
    cap = cv2.VideoCapture(str(video_path))
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    max_frames = total_frames if max_split_frames == -1 else min(max_split_frames, total_frames)

    frame_count = 0
    while cap.isOpened() and frame_count < max_frames:
        if frame_count % 100 == 0:
            print(f"æ‹†å¸§è¿›åº¦ï¼š{frame_count}/{max_frames}")
            
        ret, frame = cap.read()
        if not ret:
            break

        frame_count += 1
        # ä¿å­˜åŸå§‹å¸§
        cv2.imwrite(str(images_dir / f"{frame_count:06d}.jpg"), frame)

    cap.release()
    print(f"æ‹†å¸§å®Œæˆï¼Œå…±{max_frames}å¸§")

    # ç›®æ ‡æ£€æµ‹
    for frame_num in range(1, frame_count + 1):
        if frame_num % 100 == 0:
            print(f"æ£€æµ‹è¿›åº¦ï¼š{frame_num}/{max_frames}")

        frame = cv2.imread(str(images_dir / f"{frame_num:06d}.jpg"))
        img_height, img_width = frame.shape[:2]

        # æ¨¡å‹æ£€æµ‹
        result = det_model(frame, device=device, conf=conf, iou=iou, 
                          imgsz=imgsz, max_det=max_det, classes=classes, verbose=False)[0]
        
        # å¤„ç†æ£€æµ‹ç»“æœ
        class_ids = result.boxes.cls.int().tolist() if result.boxes.cls.numel() > 0 else []
        yolo_lines = convert_to_yolo_format(result.boxes.xyxy.cpu().numpy().tolist(), 
                                           img_width, img_height, class_ids) if class_ids else []

        # ä¿å­˜æ ‡æ³¨ç»“æœ
        with open(labels_dir / f"{frame_num:06d}.txt", 'w') as f:
            f.write('\n'.join(yolo_lines))

    return frame_count


def fill_missing_detections(labels_dir: Path, max_frames: int):
    """
    å¤„ç†æ— æ£€æµ‹ç»“æœçš„å¸§ï¼šé€å¸§æ£€æŸ¥ï¼Œä»…å½“ç©ºå¸§çš„å‰åäº”å¸§å‡å­˜åœ¨æœ‰æ•ˆæ£€æµ‹ç»“æœæ—¶æ‰è¡¥å……
    åœ¨æ£€æµ‹å®Œæˆåã€åˆ†å‰²å¼€å§‹å‰æ‰§è¡Œ
    """
    success_count = 0
    fail_count = 0

    # é€å¸§æ£€æŸ¥ï¼ˆä»1åˆ°æœ€å¤§å¸§æ•°ï¼‰
    for frame_num in range(1, max_frames + 1):
        label_path = labels_dir / f"{frame_num:06d}.txt"
        
        # æ£€æŸ¥å½“å‰å¸§æ˜¯å¦ä¸ºç©ºæ ‡æ³¨ï¼ˆä¸å­˜åœ¨æˆ–å¤§å°ä¸º0ï¼‰
        if label_path.exists() and label_path.stat().st_size > 0:
            continue  # éç©ºå¸§ç›´æ¥è·³è¿‡
        
        # æŸ¥æ‰¾æœ€è¿‘çš„æœ‰æ•ˆå‰å¸§ï¼ˆå‰5å¸§å†…ï¼‰
        closest_prev = None
        for offset in range(1, 6):
            prev_num = frame_num - offset
            if prev_num < 1:
                break  # è¶…å‡ºèµ·å§‹èŒƒå›´
            prev_path = labels_dir / f"{prev_num:06d}.txt"
            if prev_path.exists() and prev_path.stat().st_size > 0:
                closest_prev = prev_num
                break  # æ‰¾åˆ°æœ€è¿‘çš„æœ‰æ•ˆå‰å¸§å³åœæ­¢

        # æŸ¥æ‰¾æœ€è¿‘çš„æœ‰æ•ˆåå¸§ï¼ˆå5å¸§å†…ï¼‰
        closest_next = None
        for offset in range(1, 6):
            next_num = frame_num + offset
            if next_num > max_frames:
                break  # è¶…å‡ºæœ€å¤§èŒƒå›´
            next_path = labels_dir / f"{next_num:06d}.txt"
            if next_path.exists() and next_path.stat().st_size > 0:
                closest_next = next_num
                break  # æ‰¾åˆ°æœ€è¿‘çš„æœ‰æ•ˆåå¸§å³åœæ­¢

        # ä»…å½“å‰åå‡æœ‰æœ‰æ•ˆå¸§æ—¶æ‰è¡¥å……æ ‡æ³¨ï¼ˆä¼˜å…ˆä½¿ç”¨å‰å¸§ï¼‰
        if closest_prev is not None and closest_next is not None:
            with open(labels_dir / f"{closest_prev:06d}.txt", 'r') as f_prev, \
                 open(label_path, 'w') as f_curr:
                f_curr.write(f_prev.read())
            success_count += 1
        else:
            fail_count += 1
            # æ§åˆ¶è­¦å‘Šè¾“å‡ºé¢‘ç‡ï¼Œé¿å…åˆ·å±
            if fail_count <= 10 or fail_count % 100 == 0:
                print(f"è­¦å‘Šï¼šå¸§{frame_num}å‰å5å¸§æœªåŒæ—¶å­˜åœ¨æœ‰æ•ˆæ ‡æ³¨ï¼Œæ— æ³•è¡¥å……ï¼ˆç´¯è®¡{fail_count}ä¸ªï¼‰")

    print(f"ç¼ºå¤±æ ‡æ³¨å¤„ç†å®Œæˆï¼šæˆåŠŸè¡¥å……{success_count}å¸§ï¼Œæ— æ³•è¡¥å……{fail_count}å¸§")


def segment_frames(
    video_path: Union[str, Path],
    sam_model: SAM,
    input_dir: Path,
    output_path: Path,
    device: str,
    num_sam_frames: int,
    max_split_frames: int,
    color: tuple,
    alpha: float
):
    """åŸºäºæ£€æµ‹ç»“æœè¿›è¡Œåˆ†å‰²å¹¶ç”Ÿæˆå¸¦æ©ç çš„è§†é¢‘ï¼ˆä½¿ç”¨é¢„å¤„ç†åçš„æ ‡æ³¨ï¼‰"""
    cap = cv2.VideoCapture(str(video_path))
    fps = cap.get(cv2.CAP_PROP_FPS)
    width, height = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)), int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
    fourcc = cv2.VideoWriter_fourcc(*'mp4v')
    out = cv2.VideoWriter(str(output_path), fourcc, fps, (width, height))

    # ç¡®å®šéœ€è¦å¤„ç†çš„å¸§æ•°
    process_frames = max_split_frames if num_sam_frames == -1 else min(num_sam_frames, max_split_frames)
    images_dir, labels_dir = input_dir / "images", input_dir / "labels"

    for frame_num in range(1, process_frames + 1):
        if frame_num % 100 == 0:
            print(f"åˆ†å‰²è¿›åº¦ï¼š{frame_num}/{process_frames}")

        # è¯»å–å¸§å’Œé¢„å¤„ç†åçš„æ£€æµ‹ç»“æœ
        frame = cv2.imread(str(images_dir / f"{frame_num:06d}.jpg"))
        img_height, img_width = frame.shape[:2]
        
        with open(labels_dir / f"{frame_num:06d}.txt", 'r') as f:
            yolo_lines = f.readlines()

        boxes, class_ids = convert_yolo_to_boxes(yolo_lines, img_width, img_height)

        # æ‰§è¡Œåˆ†å‰²å¹¶æ¸²æŸ“
        rendered_frame = frame.copy()
        if class_ids:
            # SAMåˆ†å‰²
            sam_results = sam_model(frame, bboxes=np.array(boxes), verbose=False, save=False, device=device)
            segments = sam_results[0].masks.xyn

            # æ¸²æŸ“æ©ç 
            overlay = rendered_frame.copy()
            bgr_color = (color[2], color[1], color[0])  # RGBè½¬BGR
            for segment in segments:
                segment = (segment * np.array([img_width, img_height])).astype(np.int32)
                try:
                    cv2.fillPoly(overlay, [segment], bgr_color)
                except cv2.error as e:
                    print(f"ç¬¬{frame_num}å¸§ç»˜åˆ¶å¼‚å¸¸: {e}")

            # æ··åˆå›¾å±‚
            cv2.addWeighted(overlay, alpha, rendered_frame, 1 - alpha, 0, rendered_frame)
        else:
            print(f"ç¬¬{frame_num}å¸§æ— æœ‰æ•ˆæ ‡æ³¨ï¼Œç›´æ¥ä½¿ç”¨åŸå§‹å¸§")

        out.write(rendered_frame)

    out.release()
    cap.release()


def process_video(
    data: Union[str, Path],
    det_model: str,
    sam_model: str,
    device: str = "cuda:0",
    conf: float = 0.25,
    iou: float = 0.45,
    imgsz: int = 640,
    max_det: int = 1,
    classes: Optional[List[int]] = None,
    output_dir: Optional[Union[str, Path]] = None,
    num_sam_frames: int = -1,
    max_split_frames: int = -1,
    mask_color: tuple = (255, 0, 255),  # æ©ç é¢œè‰²(RGB)
    mask_alpha: float = 0.35  # æ©ç é€æ˜åº¦
) -> None:
    """å¤„ç†è§†é¢‘çš„ä¸»å‡½æ•°ï¼šæ‹†å¸§->æ£€æµ‹->è¡¥å……ç¼ºå¤±æ ‡æ³¨->åˆ†å‰²"""
    start_time = datetime.now()
    data = Path(data)
    
    # åˆå§‹åŒ–æ¨¡å‹
    det_model = YOLO(det_model)
    sam_model = SAM(sam_model)

    # é…ç½®è¾“å‡ºç›®å½•
    if not output_dir:
        output_dir = data.parent / f"{data.stem}_processed"
    output_dir = Path(output_dir)
    batch_dir = output_dir / get_beijing_time().strftime("%Y-%m-%d_%H-%M")
    batch_dir.mkdir(exist_ok=True, parents=True)

    # 1. æ‹†å¸§ä¸æ£€æµ‹
    frame_count = split_and_detect(
        video_path=data,
        det_model=det_model,
        output_dir=batch_dir,
        device=device,
        conf=conf,
        iou=iou,
        imgsz=imgsz,
        max_det=max_det,
        classes=classes,
        max_split_frames=max_split_frames
    )

    # 2. è¡¥å……ç¼ºå¤±çš„æ£€æµ‹ç»“æœï¼ˆæ£€æµ‹åã€åˆ†å‰²å‰æ‰§è¡Œï¼‰
    labels_dir = batch_dir / "labels"
    fill_missing_detections(labels_dir, frame_count)

    # 3. æ‰§è¡Œåˆ†å‰²å¹¶ç”Ÿæˆç»“æœè§†é¢‘
    output_video = batch_dir / f"{data.stem}_{get_beijing_time().strftime('%Y%m%d%H%M%S')}_segmented.mp4"
    segment_frames(
        video_path=data,
        sam_model=sam_model,
        input_dir=batch_dir,
        output_path=output_video,
        device=device,
        num_sam_frames=num_sam_frames,
        max_split_frames=frame_count,
        color=mask_color,
        alpha=mask_alpha
    )

    # è¾“å‡ºæ‰§è¡Œä¿¡æ¯
    elapsed_time = datetime.now() - start_time
    print(f"å¤„ç†å®Œæˆï¼å…±å¤„ç†{frame_count}å¸§ï¼Œè€—æ—¶{elapsed_time}ï¼Œç»“æœä¿å­˜è‡³ï¼š{batch_dir}")


if __name__ == "__main__":
    # é…ç½®å‚æ•°
    data_path = r"C:\Users\liuzhuo\Workspace\project\1017-chaojia-cat\æµ·è¾¹åµæ¶çš„çŒ«.mp4"
    output_dir = r"C:\Users\liuzhuo\Workspace\project\1017-chaojia-cat"
    
    det_model_name = r"C:\Users\liuzhuo\Workspace\model\yolo11x.pt"
    sam_model_name = r"C:\Users\liuzhuo\Workspace\model\sam2_l.pt"
    
    # æ£€æµ‹å‚æ•°
    conf = 0.45
    classes = [14, 15, 16]
    max_det = 2
    max_split_frames = -1  # æ‹†å¸§æ•°é‡ï¼Œ-1è¡¨ç¤ºå…¨éƒ¨
    num_sam_frames = -1    # åˆ†å‰²æ•°é‡ï¼Œ-1è¡¨ç¤ºå…¨éƒ¨
    
    # æ¸²æŸ“å‚æ•°
    mask_color = (255, 0, 255)  # RGBæ ¼å¼
    mask_alpha = 0.35

    # æ‰§è¡Œå¤„ç†
    process_video(
        data=data_path,
        det_model=det_model_name,
        sam_model=sam_model_name,
        output_dir=output_dir,
        conf=conf,
        max_det=max_det,
        classes=classes,
        num_sam_frames=num_sam_frames,
        max_split_frames=max_split_frames,
        mask_color=mask_color,
        mask_alpha=mask_alpha
    )