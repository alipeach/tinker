# Ultralytics ğŸš€ AGPL - 3.0 License - https://ultralytics.com/license

import json
from pathlib import Path
from typing import List, Optional, Union, Dict, Tuple
import cv2
import numpy as np
from ultralytics import SAM, YOLO, RTDETR
from datetime import datetime, timedelta


def get_beijing_time():
    """
    è·å–æœ¬åœ°æ—¶é—´åŠ å…«å°æ—¶
    """
    local_time = datetime.now()
    beijing_time = local_time + timedelta(hours=8)
    return beijing_time


def convert_to_yolo_format(boxes, img_width, img_height, class_ids):
    """
    å°†æ£€æµ‹ç»“æœè½¬æ¢ä¸ºYOLOæ ‡æ³¨æ ¼å¼
    """
    yolo_lines = []
    for i, box in enumerate(boxes):
        x_center = (box[0] + box[2]) / (2 * img_width)
        y_center = (box[1] + box[3]) / (2 * img_height)
        width = (box[2] - box[0]) / img_width
        height = (box[3] - box[1]) / img_height
        yolo_lines.append(f"{class_ids[i]} {x_center} {y_center} {width} {height}")
    return yolo_lines


def split_and_detect(
    data: Union[str, Path],
    det_model: RTDETR,
    batch_dir: Path,
    device: str,
    conf: float,
    iou: float,
    imgsz: int,
    max_det: int,
    classes: Optional[List[int]],
    max_split_frames: int
):
    """
    æ‹†å¸§å¹¶è¿›è¡Œç›®æ ‡æ£€æµ‹
    """
    # åˆ›å»ºä¸åŒçš„å­ç›®å½•
    images_dir = batch_dir / "images"
    images_dir.mkdir(exist_ok=True)
    labels_dir = batch_dir / "labels"
    labels_dir.mkdir(exist_ok=True)

    # å¤„ç†è§†é¢‘æ–‡ä»¶ï¼Œæ‹†å¸§ä¿å­˜åŸå§‹æ–‡ä»¶
    cap = cv2.VideoCapture(str(data))
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))

    if max_split_frames == -1:
        max_split_frames = total_frames
    else:
        max_split_frames = min(max_split_frames, total_frames)

    frame_count = 0
    while cap.isOpened() and frame_count < max_split_frames:
        if frame_count % 100 == 0:
            print(f"å½“å‰æ‹†å¸§è¿›åº¦ï¼šå·²æ‹†åˆ† {frame_count} å¸§ï¼Œå…± {max_split_frames} å¸§ã€‚")
        ret, frame = cap.read()
        if not ret:
            break

        frame_count += 1

        # ä¿å­˜åŸå§‹å¸§
        raw_frame_path = images_dir / f"{frame_count:06d}.jpg"
        cv2.imwrite(str(raw_frame_path), frame)

    cap.release()
    print(f"æ‹†å¸§å®Œæˆï¼Œå…±æ‹†äº† {max_split_frames} å¸§ã€‚å³å°†å¼€å§‹æ£€æµ‹ã€‚")

    for frame_num in range(1, frame_count + 1):
        if frame_num % 100 == 0:
            print(f"å½“å‰æ£€æµ‹è¿›åº¦ï¼šå·²æ£€æµ‹ {frame_num} å¸§ï¼Œå…± {max_split_frames} å¸§ã€‚")

        raw_frame_path = images_dir / f"{frame_num:06d}.jpg"
        frame = cv2.imread(str(raw_frame_path))
        img_height, img_width = frame.shape[:2]

        # ä½¿ç”¨ detect æ–¹æ³•è¿›è¡Œç›®æ ‡æ£€æµ‹ï¼Œå…³é—­æ—¥å¿—è¾“å‡º
        result = det_model(frame, device=device, conf=conf, iou=iou, imgsz=imgsz, max_det=max_det, classes=classes, verbose=False)[0]
        class_ids = result.boxes.cls.int().tolist() if result.boxes.cls.numel() > 0 else []
        confidences = result.boxes.conf.cpu().numpy().tolist() if result.boxes.conf.numel() > 0 else []

        if class_ids:
            boxes = result.boxes.xyxy.cpu().numpy().tolist()

            # ä¿å­˜æ£€æµ‹ç»“æœåˆ°YOLOæ ‡æ³¨æ ¼å¼
            yolo_lines = convert_to_yolo_format(boxes, img_width, img_height, class_ids)
            label_path = labels_dir / f"{frame_num:06d}.txt"
            with open(label_path, 'w') as f:
                f.write('\n'.join(yolo_lines))
        else:
            # æ²¡æœ‰æ£€æµ‹ç»“æœï¼Œä¿å­˜ä¸ºç©ºæ–‡ä»¶
            label_path = labels_dir / f"{frame_num:06d}.txt"
            with open(label_path, 'w') as f:
                pass

    return frame_count


def convert_yolo_to_boxes(yolo_lines, img_width, img_height) -> Tuple[List, List]:
    """
    å°†YOLOæ ‡æ³¨æ ¼å¼è½¬æ¢ä¸ºè¾¹ç•Œæ¡†åæ ‡
    """
    boxes = []
    class_ids = []
    for line in yolo_lines:
        parts = line.strip().split()
        class_id = int(parts[0])
        x_center = float(parts[1])
        y_center = float(parts[2])
        width = float(parts[3])
        height = float(parts[4])

        x1 = (x_center - width / 2) * img_width
        y1 = (y_center - height / 2) * img_height
        x2 = (x_center + width / 2) * img_width
        y2 = (y_center + height / 2) * img_height

        boxes.append([x1, y1, x2, y2])
        class_ids.append(class_id)
    return boxes, class_ids


def get_reusable_frame(frame_num: int, 
                       max_split_frames: int, 
                       labels_dir: Path, 
                       img_width: int, 
                       img_height: int,
                       max_offset: int = 5) -> Optional[Dict]:
    """
    æŸ¥æ‰¾å¯å¤ç”¨çš„å‰åå¸§æ£€æµ‹ç»“æœ
    
    åªæœ‰å‰åå¸§éƒ½å­˜åœ¨æœ‰æ•ˆæ£€æµ‹ç»“æœæ—¶æ‰è¿”å›å¤ç”¨ç»“æœï¼Œé»˜è®¤è¿”å›æœ€è¿‘çš„å‰ä¸€å¸§
    
    Args:
        frame_num: å½“å‰å¸§ç¼–å·
        max_split_frames: æœ€å¤§å¸§æ•°
        labels_dir: æ ‡ç­¾æ–‡ä»¶ç›®å½•
        img_width: å›¾åƒå®½åº¦
        img_height: å›¾åƒé«˜åº¦
        max_offset: æœ€å¤§æŸ¥æ‰¾åç§»é‡ï¼Œé»˜è®¤5å¸§
        
    Returns:
        åŒ…å«boxeså’Œclass_idsçš„å­—å…¸ï¼Œæ— å¯ç”¨å¸§æ—¶è¿”å›None
    """
    # æŸ¥æ‰¾å‰å‘å¸§ï¼ˆæœ€è¿‘çš„æœ‰æ•ˆå‰å¸§ï¼‰
    prev_result = None
    for offset in range(1, max_offset + 1):
        prev_frame_num = frame_num - offset
        if prev_frame_num < 1:
            break
            
        prev_label_path = labels_dir / f"{prev_frame_num:06d}.txt"
        with open(prev_label_path, 'r') as f:
            prev_yolo_lines = f.readlines()
            prev_boxes, prev_class_ids = convert_yolo_to_boxes(prev_yolo_lines, img_width, img_height)
            if prev_class_ids:
                prev_result = {"boxes": prev_boxes, "class_ids": prev_class_ids}
                break  # æ‰¾åˆ°æœ€è¿‘çš„å‰å‘æœ‰æ•ˆå¸§å°±åœæ­¢
    
    # æŸ¥æ‰¾åå‘å¸§ï¼ˆæœ€è¿‘çš„æœ‰æ•ˆåå¸§ï¼‰
    next_result = None
    for offset in range(1, max_offset + 1):
        next_frame_num = frame_num + offset
        if next_frame_num > max_split_frames:
            break
            
        next_label_path = labels_dir / f"{next_frame_num:06d}.txt"
        with open(next_label_path, 'r') as f:
            next_yolo_lines = f.readlines()
            next_boxes, next_class_ids = convert_yolo_to_boxes(next_yolo_lines, img_width, img_height)
            if next_class_ids:
                next_result = {"boxes": next_boxes, "class_ids": next_class_ids}
                break  # æ‰¾åˆ°æœ€è¿‘çš„åå‘æœ‰æ•ˆå¸§å°±åœæ­¢
    
    # åªæœ‰å‰åéƒ½æœ‰æœ‰æ•ˆå¸§æ—¶æ‰è¿”å›å¤ç”¨ç»“æœï¼Œä¼˜å…ˆä½¿ç”¨å‰å‘å¸§
    if prev_result and next_result:
        return prev_result
    return None


def segment_frames(
    data: Union[str, Path],
    sam_model: SAM,
    batch_dir: Path,
    device: str,
    num_sam_frames: int,
    max_split_frames: int
):
    """
    åŸºäºæ‹†å¸§å’Œæ£€æµ‹ç»“æœè¿›è¡Œåˆ†å‰²
    é€»è¾‘ä¼˜å…ˆçº§ï¼šå½“å‰å¸§æ£€æµ‹ç»“æœ > å‰åå¸§å¤ç”¨ç»“æœ > å…¨ç›®æ ‡é¢œè‰²è¦†ç›–
    """
    cap = cv2.VideoCapture(str(data))
    fps = cap.get(cv2.CAP_PROP_FPS)
    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
    fourcc = cv2.VideoWriter_fourcc(*'mp4v')

    timestamp = get_beijing_time().strftime("%Y%m%d%H%M%S")
    output_video_path = batch_dir / f"{data.stem}_{timestamp}_segmented.mp4"
    out = cv2.VideoWriter(str(output_video_path), fourcc, fps, (width, height))

    if num_sam_frames == -1:
        num_sam_frames = max_split_frames
    else:
        num_sam_frames = min(num_sam_frames, max_split_frames)

    images_dir = batch_dir / "images"
    labels_dir = batch_dir / "labels"

    # å®šä¹‰ç›®æ ‡é¢œè‰²ï¼ˆBGRæ ¼å¼ï¼‰
    bgr_color = (128, 0, 128)  # ç´«è‰²
    # å¯æ›¿æ¢ä¸ºå…¶ä»–é¢œè‰²ï¼š
    # bgr_color = (0, 255, 0)    # ç»¿è‰²
    # bgr_color = (0, 0, 255)    # çº¢è‰²
    alpha = 0.35  # æœ‰åˆ†å‰²æ—¶çš„å¤–å›´é¢œè‰²é€æ˜åº¦

    empty_frames = []  # è®°å½•æ— æ£€æµ‹ç»“æœä¸”æ— å¯å¤ç”¨å¸§çš„å¸§å·
    for frame_num in range(1, num_sam_frames + 1):
        if frame_num % 100 == 0:
            print(f"å½“å‰åˆ†å‰²è¿›åº¦ï¼šå·²å¤„ç† {frame_num} å¸§ï¼Œå…± {num_sam_frames} å¸§ã€‚")

        raw_frame_path = images_dir / f"{frame_num:06d}.jpg"
        frame = cv2.imread(str(raw_frame_path))
        img_height, img_width = frame.shape[:2]

        # 1. åŠ è½½å½“å‰å¸§æ£€æµ‹ç»“æœ
        label_path = labels_dir / f"{frame_num:06d}.txt"
        with open(label_path, 'r') as f:
            yolo_lines = f.readlines()
        boxes, class_ids = convert_yolo_to_boxes(yolo_lines, img_width, img_height)

        # 2. æ— æ£€æµ‹ç»“æœæ—¶ï¼Œå°è¯•å¤ç”¨å‰åå¸§ç»“æœ
        reusable_frame_num = None
        if not class_ids:
            reusable_result = get_reusable_frame(
                frame_num, max_split_frames, labels_dir, img_width, img_height
            )
            if reusable_result:
                boxes = reusable_result["boxes"]
                class_ids = reusable_result["class_ids"]
                # è®°å½•å¤ç”¨çš„å¸§å·ï¼ˆç”¨äºæ—¥å¿—ï¼‰
                reusable_frame_num = frame_num - 1  # å¤ç”¨çš„æ˜¯æœ€è¿‘å‰å¸§
                print(f"å¸§ {frame_num} æ— æ£€æµ‹ç»“æœï¼Œå¤ç”¨å¸§ {reusable_frame_num} çš„æ£€æµ‹ç»“æœ")
            else:
                empty_frames.append(frame_num)

        # 3. æ¸²æŸ“é€»è¾‘
        if class_ids:
            # æœ‰æ£€æµ‹ç»“æœ/å¤ç”¨æˆåŠŸï¼šæ‰§è¡Œæ­£å¸¸åˆ†å‰²æ¸²æŸ“
            if empty_frames:
                print(f"å¸§ {', '.join(map(str, empty_frames))} æ— æ£€æµ‹ç»“æœä¸”æ— å¯å¤ç”¨å¸§ï¼Œä½¿ç”¨å…¨ç›®æ ‡é¢œè‰²")
                empty_frames = []
            
            boxes = np.array(boxes)
            try:
                sam_results = sam_model(frame, bboxes=boxes, verbose=False, save=False, device=device)
                segments = sam_results[0].masks.xyn if sam_results[0].masks is not None else []
            except Exception as e:
                print(f"å¸§ {frame_num} åˆ†å‰²å¼‚å¸¸ï¼š{e}ï¼Œä½¿ç”¨å…¨ç›®æ ‡é¢œè‰²")
                segments = []

            # åˆ›å»ºè¦†ç›–å±‚å’Œæ©ç 
            overlay = np.full_like(frame, bgr_color, dtype=np.uint8)
            mask = np.zeros((img_height, img_width), dtype=np.uint8)
            
            for segment in segments:
                segment = (segment * np.array([img_width, img_height])).astype(np.int32)
                try:
                    cv2.fillPoly(mask, [segment], 255)
                except cv2.error as e:
                    print(f"å¸§ {frame_num} æ©ç ç»˜åˆ¶å¼‚å¸¸ï¼š{e}")
                    continue

            # æ··åˆåˆ†å‰²åŒºåŸŸå’Œå¤–å›´é¢œè‰²
            segmented_area = cv2.bitwise_and(frame, frame, mask=mask)
            non_segment_mask = cv2.bitwise_not(mask)
            non_segmented_area = cv2.bitwise_and(overlay, overlay, mask=non_segment_mask)
            rendered_frame = cv2.addWeighted(segmented_area, 1.0, non_segmented_area, 1 - alpha, 0)

        else:
            # æ— æ£€æµ‹ç»“æœä¸”æ— å¯å¤ç”¨å¸§ï¼šç›´æ¥ä½¿ç”¨å…¨ç›®æ ‡é¢œè‰²
            rendered_frame = np.full_like(frame, bgr_color, dtype=np.uint8)

        out.write(rendered_frame)

    # è¾“å‡ºå‰©ä½™æ— æ£€æµ‹ç»“æœä¸”æ— å¯å¤ç”¨å¸§çš„æ—¥å¿—
    if empty_frames:
        print(f"å¸§ {', '.join(map(str, empty_frames))} æ— æ£€æµ‹ç»“æœä¸”æ— å¯å¤ç”¨å¸§ï¼Œä½¿ç”¨å…¨ç›®æ ‡é¢œè‰²")

    out.release()
    cap.release()
    print(f"åˆ†å‰²å®Œæˆï¼Œè¾“å‡ºè§†é¢‘è·¯å¾„ï¼š{output_video_path}")


def auto_annotate(
    data: Union[str, Path],
    det_model: str = "/workspace/yolo11x.pt",
    sam_model: str = "/workspace/sam_b.pt",
    device: str = "cuda:0",
    conf: float = 0.25,
    iou: float = 0.45,
    imgsz: int = 640,
    max_det: int = 2,
    classes: Optional[List[int]] = [15],
    output_dir: Optional[Union[str, Path]] = "/workspace/outputs",
    num_sam_frames: int = -1,
    max_split_frames: int = -1,
    mode: str = "detect_and_segment"
) -> None:
    """
    Automatically annotate a video using a YOLO object detection model and a SAM segmentation model.

    This function processes frames of a video, detects objects using a YOLO model,
    and then generates segmentation masks using a SAM model. The resulting masks are rendered on a new video.

    Args:
        data (str | Path): Path to a video file to be annotated.
        det_model (str): Path or name of the pre - trained YOLO detection model.
        sam_model (str): Path or name of the pre - trained SAM segmentation model.
        device (str): Device to run the models on (e.g., 'cpu', 'cuda', '0').
        conf (float): Confidence threshold for detection model.
        iou (float): IoU threshold for filtering overlapping boxes in detection results.
        imgsz (int): Input image resize dimension.
        max_det (int): Maximum number of detections per image.
        classes (List[int] | None): Filter predictions to specified class IDs, returning only relevant detections.
        output_dir (str | Path | None): Directory to save the annotated results. If None, a default directory is created.
        num_sam_frames (int): Number of frames to process for SAM segmentation after detection. -1 means process all detected frames.
        max_split_frames (int): Maximum number of frames to split. -1 means split all frames.
        mode (str): Mode of operation. Can be 'detect', 'segment', or 'detect_and_segment'.
    """
    start_time = datetime.now()
    det_model = YOLO(det_model)
    sam_model = SAM(sam_model)

    data = Path(data)
    if not output_dir:
        output_dir = data.parent / f"{data.stem}_auto_annotate_labels"
    output_dir = Path(output_dir)

    # è·å–æœ¬åœ°æ—¶é—´åŠ å…«å°æ—¶å¹¶ç”Ÿæˆæ‰¹æ¬¡å·ç›®å½•ï¼Œç²¾ç¡®åˆ°åˆ†é’Ÿ
    beijing_time = get_beijing_time()
    batch_dir_name = beijing_time.strftime("%Y-%m-%d_%H-%M")
    batch_dir = output_dir / batch_dir_name
    batch_dir.mkdir(exist_ok=True, parents=True)

    frame_count = 0
    if mode in ["detect", "detect_and_segment"]:
        # æ‹†å¸§å¹¶æ£€æµ‹
        frame_count = split_and_detect(
            data, det_model, batch_dir, device, conf, iou, imgsz, max_det, classes, max_split_frames
        )

    if mode in ["segment", "detect_and_segment"]:
        if frame_count == 0:
            # å¦‚æœåªè¿›è¡Œåˆ†å‰²ï¼Œéœ€è¦è®¡ç®—æœ€å¤§å¸§æ•°
            cap = cv2.VideoCapture(str(data))
            total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
            if max_split_frames == -1:
                frame_count = total_frames
            else:
                frame_count = min(max_split_frames, total_frames)
            cap.release()
        # åˆ†å‰²
        segment_frames(data, sam_model, batch_dir, device, num_sam_frames, frame_count)

    end_time = datetime.now()
    elapsed_time = end_time - start_time
    log_message = f"ç¨‹åºæ‰§è¡Œå®Œæˆï¼Œå¤„ç†äº† {frame_count} å¸§ï¼Œè€—æ—¶ {elapsed_time}ã€‚"
    print(log_message)


if __name__ == "__main__":
    # è¯·æ ¹æ®å®é™…æƒ…å†µä¿®æ”¹è¿™äº›å‚æ•°
    data_path = r"C:\Users\xxxx\Workspace\project\1119-happy-cat\happy-cat.mp4"
    output_dir = r"C:\Users\xxxx\Workspace\project\1119-happy-cat"
    
    det_model_name = r"C:\Users\xxxx\Workspace\model\yolo11x.pt"
    sam_model_name = r"C:\Users\xxxx\Workspace\model\sam2_l.pt"

    conf = 0.25
    classes = [14, 15, 16]  # æ£€æµ‹ç±»åˆ«ï¼ˆ14: ç‹—, 15: çŒ«, 16: é©¬ï¼‰
    num_sam_frames = -1  # å¤„ç†æ‰€æœ‰å¸§
    max_split_frames = -1  # æ‹†åˆ†æ‰€æœ‰å¸§
    mode = 'detect_and_segment'  # æ£€æµ‹å¹¶åˆ†å‰²æ¨¡å¼

    auto_annotate(
        data=data_path,
        det_model=det_model_name,
        sam_model=sam_model_name,
        output_dir=output_dir,
        conf=conf,
        classes=classes,
        num_sam_frames=num_sam_frames,
        max_split_frames=max_split_frames,
        mode=mode
    )